/home/jinyao/anaconda3/envs/IADBE/bin/python /home/jinyao/PycharmProjects/IADBE/train_test_btech_reverse_distillation.py
2024-06-27 10:45:14,741 - INFO - ================== Processing dataset: 01 ==================
2024-06-27 10:45:14,741 - INFO - Initializing ReverseDistillation model.
2024-06-27 10:45:14,742 - INFO - ================== Start training for dataset: 01 ==================
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
2024-06-27 10:45:14,828 - WARNING - No implementation of `configure_transforms` was provided in the Lightning model. Using default transforms from the base class. This may not be suitable for your use case. Please override `configure_transforms` in your model.
/home/jinyao/anaconda3/envs/IADBE/lib/python3.10/site-packages/lightning/pytorch/loops/utilities.py:73: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.
2024-06-27 10:45:14,829 - INFO - Found the dataset.
You are using a CUDA device ('NVIDIA GeForce RTX 3090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
2024-06-27 10:45:16,062 - INFO - Loading pretrained weights from url (https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/wide_resnet50_racm-8234f177.pth)
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
┏━━━┳━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃   ┃ Name                  ┃ Type                     ┃ Params ┃
┡━━━╇━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0 │ loss                  │ ReverseDistillationLoss  │      0 │
│ 1 │ _transform            │ Compose                  │      0 │
│ 2 │ normalization_metrics │ MinMax                   │      0 │
│ 3 │ image_threshold       │ F1AdaptiveThreshold      │      0 │
│ 4 │ pixel_threshold       │ F1AdaptiveThreshold      │      0 │
│ 5 │ image_metrics         │ AnomalibMetricCollection │      0 │
│ 6 │ pixel_metrics         │ AnomalibMetricCollection │      0 │
│ 7 │ model                 │ ReverseDistillationModel │ 89.0 M │
└───┴───────────────────────┴──────────────────────────┴────────┘
Trainable params: 89.0 M
Non-trainable params: 0
Total params: 89.0 M
Total estimated model params size (MB): 356
/home/jinyao/anaconda3/envs/IADBE/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=63` in the `DataLoader` to improve performance.
/home/jinyao/anaconda3/envs/IADBE/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=63` in the `DataLoader` to improve performance.
/home/jinyao/anaconda3/envs/IADBE/lib/python3.10/site-packages/lightning/pytorch/core/module.py:494: You called `self.log('train_loss', ..., logger=True)` but have no logger configured. You can enable one by doing `Trainer(logger=ALogger(...))`
`Trainer.fit` stopped: `max_epochs=1000` reached.
2024-06-27 20:05:07,720 - INFO - Training took 33590.85 seconds
2024-06-27 20:05:07,723 - INFO - ================== Start testing for dataset: 01 ==================
Epoch 999/999 ━━━━━━━━━━━━━━━━ 13/13 0:00:25 •        0.50it/s train_loss_step:
                                     0:00:00                   0.025
                                                               pixel_AUROC:
                                                               0.974 pixel_PRO:
                                                               0.421
                                                               train_loss_epoch:
                                                               0.025
2024-06-27 20:05:08,074 - INFO - Found the dataset.
Restoring states from the checkpoint path at /home/jinyao/PycharmProjects/IADBE/results/ReverseDistillation/BTech/01/v0/weights/lightning/model.ckpt
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
Loaded model weights from the checkpoint at /home/jinyao/PycharmProjects/IADBE/results/ReverseDistillation/BTech/01/v0/weights/lightning/model.ckpt
/home/jinyao/anaconda3/envs/IADBE/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=63` in the `DataLoader` to improve performance.
2024-06-27 20:05:27,867 - INFO - Testing took 19.129489183425903 seconds
Throughput (batch_size=32) : 3.659271783412237 FPS
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃        Test metric        ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│        image_AUROC        │    0.9961127638816833     │
│         image_PRO         │            1.0            │
│        pixel_AUROC        │    0.9738993644714355     │
│         pixel_PRO         │    0.4214225113391876     │
└───────────────────────────┴───────────────────────────┘
Testing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3/3 0:00:14 • 0:00:00 0.14it/s
2024-06-27 20:05:28,052 - INFO - ================== Processing dataset: 02 ==================
2024-06-27 20:05:28,052 - INFO - Initializing ReverseDistillation model.
2024-06-27 20:05:28,052 - INFO - ================== Start training for dataset: 02 ==================
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
2024-06-27 20:05:28,085 - WARNING - No implementation of `configure_transforms` was provided in the Lightning model. Using default transforms from the base class. This may not be suitable for your use case. Please override `configure_transforms` in your model.
/home/jinyao/anaconda3/envs/IADBE/lib/python3.10/site-packages/lightning/pytorch/loops/utilities.py:73: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.
2024-06-27 20:05:28,086 - INFO - Found the dataset.
2024-06-27 20:05:29,273 - INFO - Loading pretrained weights from url (https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/wide_resnet50_racm-8234f177.pth)
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
┏━━━┳━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃   ┃ Name                  ┃ Type                     ┃ Params ┃
┡━━━╇━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0 │ loss                  │ ReverseDistillationLoss  │      0 │
│ 1 │ _transform            │ Compose                  │      0 │
│ 2 │ normalization_metrics │ MinMax                   │      0 │
│ 3 │ image_threshold       │ F1AdaptiveThreshold      │      0 │
│ 4 │ pixel_threshold       │ F1AdaptiveThreshold      │      0 │
│ 5 │ image_metrics         │ AnomalibMetricCollection │      0 │
│ 6 │ pixel_metrics         │ AnomalibMetricCollection │      0 │
│ 7 │ model                 │ ReverseDistillationModel │ 89.0 M │
└───┴───────────────────────┴──────────────────────────┴────────┘
Trainable params: 89.0 M
Non-trainable params: 0
Total params: 89.0 M
Total estimated model params size (MB): 356
/home/jinyao/anaconda3/envs/IADBE/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=63` in the `DataLoader` to improve performance.
/home/jinyao/anaconda3/envs/IADBE/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=63` in the `DataLoader` to improve performance.
/home/jinyao/anaconda3/envs/IADBE/lib/python3.10/site-packages/lightning/pytorch/core/module.py:494: You called `self.log('train_loss', ..., logger=True)` but have no logger configured. You can enable one by doing `Trainer(logger=ALogger(...))`
`Trainer.fit` stopped: `max_epochs=1000` reached.
2024-06-28 00:45:46,729 - INFO - Training took 16816.69 seconds
2024-06-28 00:45:46,731 - INFO - ================== Start testing for dataset: 02 ==================
Epoch 999/999 ━━━━━━━━━━━━━━━━ 13/13 0:00:06 •        1.89it/s train_loss_step:
                                     0:00:00                   0.042
                                                               pixel_AUROC:
                                                               0.953 pixel_PRO:
                                                               0.306
                                                               train_loss_epoch:
                                                               0.041
2024-06-28 00:45:47,019 - INFO - Found the dataset.
Restoring states from the checkpoint path at /home/jinyao/PycharmProjects/IADBE/results/ReverseDistillation/BTech/02/v0/weights/lightning/model.ckpt
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
Loaded model weights from the checkpoint at /home/jinyao/PycharmProjects/IADBE/results/ReverseDistillation/BTech/02/v0/weights/lightning/model.ckpt
/home/jinyao/anaconda3/envs/IADBE/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=63` in the `DataLoader` to improve performance.
2024-06-28 00:46:23,168 - INFO - Testing took 35.57188320159912 seconds
Throughput (batch_size=32) : 6.465780816171701 FPS
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃        Test metric        ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│        image_AUROC        │    0.8624999523162842     │
│         image_PRO         │            1.0            │
│        pixel_AUROC        │    0.9676218628883362     │
│         pixel_PRO         │    0.30561724305152893    │
└───────────────────────────┴───────────────────────────┘
2024-06-28 00:46:23,400 - INFO - ================== Processing dataset: 03 ==================
2024-06-28 00:46:23,401 - INFO - Initializing ReverseDistillation model.
2024-06-28 00:46:23,401 - INFO - ================== Start training for dataset: 03 ==================
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
2024-06-28 00:46:23,411 - WARNING - No implementation of `configure_transforms` was provided in the Lightning model. Using default transforms from the base class. This may not be suitable for your use case. Please override `configure_transforms` in your model.
/home/jinyao/anaconda3/envs/IADBE/lib/python3.10/site-packages/lightning/pytorch/loops/utilities.py:73: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.
2024-06-28 00:46:23,411 - INFO - Found the dataset.
Testing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 8/8 0:00:31 • 0:00:00 0.23it/s
2024-06-28 00:46:24,554 - INFO - Loading pretrained weights from url (https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/wide_resnet50_racm-8234f177.pth)
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
┏━━━┳━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃   ┃ Name                  ┃ Type                     ┃ Params ┃
┡━━━╇━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0 │ loss                  │ ReverseDistillationLoss  │      0 │
│ 1 │ _transform            │ Compose                  │      0 │
│ 2 │ normalization_metrics │ MinMax                   │      0 │
│ 3 │ image_threshold       │ F1AdaptiveThreshold      │      0 │
│ 4 │ pixel_threshold       │ F1AdaptiveThreshold      │      0 │
│ 5 │ image_metrics         │ AnomalibMetricCollection │      0 │
│ 6 │ pixel_metrics         │ AnomalibMetricCollection │      0 │
│ 7 │ model                 │ ReverseDistillationModel │ 89.0 M │
└───┴───────────────────────┴──────────────────────────┴────────┘
Trainable params: 89.0 M
Non-trainable params: 0
Total params: 89.0 M
Total estimated model params size (MB): 356
/home/jinyao/anaconda3/envs/IADBE/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=63` in the `DataLoader` to improve performance.
/home/jinyao/anaconda3/envs/IADBE/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=63` in the `DataLoader` to improve performance.
/home/jinyao/anaconda3/envs/IADBE/lib/python3.10/site-packages/lightning/pytorch/core/module.py:494: You called `self.log('train_loss', ..., logger=True)` but have no logger configured. You can enable one by doing `Trainer(logger=ALogger(...))`
`Trainer.fit` stopped: `max_epochs=1000` reached.
Epoch 999/999 ━━━━━━━━━━━━━━━━ 32/32 0:00:18 •        1.71it/s train_loss_step:
                                     0:00:00                   0.045
                                                               pixel_AUROC:
                                                               0.997 pixel_PRO:
                                                               0.418
                                                               train_loss_epoch:
                                                               0.044
2024-06-28 11:04:27,480 - INFO - Training took 37082.16 seconds
2024-06-28 11:04:27,482 - INFO - ================== Start testing for dataset: 03 ==================
2024-06-28 11:04:27,746 - INFO - Found the dataset.
Restoring states from the checkpoint path at /home/jinyao/PycharmProjects/IADBE/results/ReverseDistillation/BTech/03/v0/weights/lightning/model.ckpt
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
Loaded model weights from the checkpoint at /home/jinyao/PycharmProjects/IADBE/results/ReverseDistillation/BTech/03/v0/weights/lightning/model.ckpt
/home/jinyao/anaconda3/envs/IADBE/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=63` in the `DataLoader` to improve performance.
2024-06-28 11:05:36,379 - INFO - Testing took 68.06679773330688 seconds
Throughput (batch_size=32) : 6.478929737930172 FPS
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃        Test metric        ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│        image_AUROC        │    0.9997560977935791     │
│         image_PRO         │            1.0            │
│        pixel_AUROC        │    0.9971857666969299     │
│         pixel_PRO         │    0.41810423135757446    │
└───────────────────────────┴───────────────────────────┘
Testing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 14/14 0:00:59 • 0:00:00 0.22it/s