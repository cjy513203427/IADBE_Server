/home/jinyao/anaconda3/envs/IADBE/bin/python /home/jinyao/PycharmProjects/IADBE/train_test_visa_reverse_distillation.py
2024-06-12 16:43:33,161 - INFO - ================== Processing dataset: candle ==================
2024-06-12 16:43:33,161 - INFO - Initializing ReverseDistillation model.
2024-06-12 16:43:33,162 - INFO - ================== Start training for dataset: candle ==================
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
2024-06-12 16:43:33,258 - WARNING - No implementation of `configure_transforms` was provided in the Lightning model. Using default transforms from the base class. This may not be suitable for your use case. Please override `configure_transforms` in your model.
2024-06-12 16:43:33,259 - INFO - Found the dataset and train/test split.
You are using a CUDA device ('NVIDIA GeForce RTX 3090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
2024-06-12 16:43:34,509 - INFO - Loading pretrained weights from url (https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/wide_resnet50_racm-8234f177.pth)
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
┏━━━┳━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃   ┃ Name                  ┃ Type                     ┃ Params ┃
┡━━━╇━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0 │ loss                  │ ReverseDistillationLoss  │      0 │
│ 1 │ _transform            │ Compose                  │      0 │
│ 2 │ normalization_metrics │ MinMax                   │      0 │
│ 3 │ image_threshold       │ F1AdaptiveThreshold      │      0 │
│ 4 │ pixel_threshold       │ F1AdaptiveThreshold      │      0 │
│ 5 │ image_metrics         │ AnomalibMetricCollection │      0 │
│ 6 │ pixel_metrics         │ AnomalibMetricCollection │      0 │
│ 7 │ model                 │ ReverseDistillationModel │ 89.0 M │
└───┴───────────────────────┴──────────────────────────┴────────┘
Trainable params: 89.0 M
Non-trainable params: 0
Total params: 89.0 M
Total estimated model params size (MB): 356
/home/jinyao/anaconda3/envs/IADBE/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=63` in the `DataLoader` to improve performance.
/home/jinyao/anaconda3/envs/IADBE/lib/python3.10/site-packages/lightning/pytorch/core/module.py:494: You called `self.log('train_loss', ..., logger=True)` but have no logger configured. You can enable one by doing `Trainer(logger=ALogger(...))`





/home/jinyao/anaconda3/envs/IADBE/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=63` in the `DataLoader` to improve performance.
`Trainer.fit` stopped: `max_epochs=200` reached.
2024-06-12 17:56:44,110 - INFO - Training took 4388.81 seconds
2024-06-12 17:56:44,112 - INFO - ================== Start testing for dataset: candle ==================
2024-06-12 17:56:44,114 - INFO - Found the dataset and train/test split.
Epoch 199/199 ━━━━━━━━━━━━━━━━ 29/29 0:00:21 •        1.35it/s train_loss_step:
                                     0:00:00                   0.049
                                                               train_loss_epoch:
                                                               0.043
                                                               pixel_AUROC:
                                                               0.990 pixel_PRO:
                                                               0.428
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/home/jinyao/anaconda3/envs/IADBE/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=63` in the `DataLoader` to improve performance.
2024-06-12 17:57:19,329 - INFO - Testing took 34.98213243484497 seconds
Throughput (batch_size=32) : 5.717204357753336 FPS
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃        Test metric        ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│        image_AUROC        │    0.9422000050544739     │
│         image_PRO         │            1.0            │
│        pixel_AUROC        │    0.9901312589645386     │
│         pixel_PRO         │    0.42785635590553284    │
└───────────────────────────┴───────────────────────────┘
2024-06-12 17:57:19,636 - INFO - ================== Processing dataset: capsules ==================
2024-06-12 17:57:19,637 - INFO - Initializing ReverseDistillation model.
2024-06-12 17:57:19,637 - INFO - ================== Start training for dataset: capsules ==================
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
2024-06-12 17:57:19,647 - WARNING - No implementation of `configure_transforms` was provided in the Lightning model. Using default transforms from the base class. This may not be suitable for your use case. Please override `configure_transforms` in your model.
2024-06-12 17:57:19,647 - INFO - Found the dataset and train/test split.
Testing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7/7 0:00:30 • 0:00:00 0.20it/s
2024-06-12 17:57:20,862 - INFO - Loading pretrained weights from url (https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/wide_resnet50_racm-8234f177.pth)
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/home/jinyao/anaconda3/envs/IADBE/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=63` in the `DataLoader` to improve performance.
┏━━━┳━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃   ┃ Name                  ┃ Type                     ┃ Params ┃
┡━━━╇━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0 │ loss                  │ ReverseDistillationLoss  │      0 │
│ 1 │ _transform            │ Compose                  │      0 │
│ 2 │ normalization_metrics │ MinMax                   │      0 │
│ 3 │ image_threshold       │ F1AdaptiveThreshold      │      0 │
│ 4 │ pixel_threshold       │ F1AdaptiveThreshold      │      0 │
│ 5 │ image_metrics         │ AnomalibMetricCollection │      0 │
│ 6 │ pixel_metrics         │ AnomalibMetricCollection │      0 │
│ 7 │ model                 │ ReverseDistillationModel │ 89.0 M │
└───┴───────────────────────┴──────────────────────────┴────────┘
Trainable params: 89.0 M
Non-trainable params: 0
Total params: 89.0 M
Total estimated model params size (MB): 356
/home/jinyao/anaconda3/envs/IADBE/lib/python3.10/site-packages/lightning/pytorch/core/module.py:494: You called `self.log('train_loss', ..., logger=True)` but have no logger configured. You can enable one by doing `Trainer(logger=ALogger(...))`
/home/jinyao/anaconda3/envs/IADBE/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=63` in the `DataLoader` to improve performance.
`Trainer.fit` stopped: `max_epochs=200` reached.
2024-06-12 18:42:10,413 - INFO - Training took 2688.80 seconds
2024-06-12 18:42:10,415 - INFO - ================== Start testing for dataset: capsules ==================
2024-06-12 18:42:10,417 - INFO - Found the dataset and train/test split.
Epoch 199/199 ━━━━━━━━━━━━━━━━ 17/17 0:00:13 •        1.25it/s train_loss_step:
                                     0:00:00                   0.109
                                                               train_loss_epoch:
                                                               0.108
                                                               pixel_AUROC:
                                                               0.996 pixel_PRO:
                                                               0.347
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/home/jinyao/anaconda3/envs/IADBE/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=63` in the `DataLoader` to improve performance.
2024-06-12 18:42:39,311 - INFO - Testing took 28.670430421829224 seconds
Throughput (batch_size=32) : 5.580662642517514 FPS
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃        Test metric        ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│        image_AUROC        │     0.875166654586792     │
│         image_PRO         │            1.0            │
│        pixel_AUROC        │    0.9955392479896545     │
│         pixel_PRO         │    0.3465484380722046     │
└───────────────────────────┴───────────────────────────┘
2024-06-12 18:42:39,536 - INFO - ================== Processing dataset: cashew ==================
2024-06-12 18:42:39,536 - INFO - Initializing ReverseDistillation model.
2024-06-12 18:42:39,537 - INFO - ================== Start training for dataset: cashew ==================
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
2024-06-12 18:42:39,547 - WARNING - No implementation of `configure_transforms` was provided in the Lightning model. Using default transforms from the base class. This may not be suitable for your use case. Please override `configure_transforms` in your model.
2024-06-12 18:42:39,548 - INFO - Found the dataset and train/test split.
Testing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5/5 0:00:21 • 0:00:00 0.19it/s
2024-06-12 18:42:40,940 - INFO - Loading pretrained weights from url (https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/wide_resnet50_racm-8234f177.pth)
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
┏━━━┳━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃   ┃ Name                  ┃ Type                     ┃ Params ┃
┡━━━╇━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0 │ loss                  │ ReverseDistillationLoss  │      0 │
│ 1 │ _transform            │ Compose                  │      0 │
│ 2 │ normalization_metrics │ MinMax                   │      0 │
│ 3 │ image_threshold       │ F1AdaptiveThreshold      │      0 │
│ 4 │ pixel_threshold       │ F1AdaptiveThreshold      │      0 │
│ 5 │ image_metrics         │ AnomalibMetricCollection │      0 │
│ 6 │ pixel_metrics         │ AnomalibMetricCollection │      0 │
│ 7 │ model                 │ ReverseDistillationModel │ 89.0 M │
└───┴───────────────────────┴──────────────────────────┴────────┘
Trainable params: 89.0 M
Non-trainable params: 0
Total params: 89.0 M
Total estimated model params size (MB): 356
/home/jinyao/anaconda3/envs/IADBE/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=63` in the `DataLoader` to improve performance.
/home/jinyao/anaconda3/envs/IADBE/lib/python3.10/site-packages/lightning/pytorch/core/module.py:494: You called `self.log('train_loss', ..., logger=True)` but have no logger configured. You can enable one by doing `Trainer(logger=ALogger(...))`
/home/jinyao/anaconda3/envs/IADBE/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=63` in the `DataLoader` to improve performance.
`Trainer.fit` stopped: `max_epochs=200` reached.
2024-06-12 19:19:52,236 - INFO - Training took 2230.65 seconds
2024-06-12 19:19:52,238 - INFO - ================== Start testing for dataset: cashew ==================
2024-06-12 19:19:52,239 - INFO - Found the dataset and train/test split.
Epoch 199/199 ━━━━━━━━━━━━━━━━ 15/15 0:00:11 •        1.34it/s train_loss_step:
                                     0:00:00                   0.104
                                                               train_loss_epoch:
                                                               0.106
                                                               pixel_AUROC:
                                                               0.954 pixel_PRO:
                                                               0.512
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/home/jinyao/anaconda3/envs/IADBE/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=63` in the `DataLoader` to improve performance.
2024-06-12 19:20:19,518 - INFO - Testing took 27.068525075912476 seconds
Throughput (batch_size=32) : 5.5414914399411 FPS
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃        Test metric        ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│        image_AUROC        │     0.968999981880188     │
│         image_PRO         │            1.0            │
│        pixel_AUROC        │    0.9536730051040649     │
│         pixel_PRO         │    0.5122141242027283     │
└───────────────────────────┴───────────────────────────┘
Testing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5/5 0:00:21 • 0:00:00 0.19it/s
2024-06-12 19:20:19,726 - INFO - ================== Processing dataset: chewinggum ==================
2024-06-12 19:20:19,726 - INFO - Initializing ReverseDistillation model.
2024-06-12 19:20:19,726 - INFO - ================== Start training for dataset: chewinggum ==================
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
2024-06-12 19:20:19,736 - WARNING - No implementation of `configure_transforms` was provided in the Lightning model. Using default transforms from the base class. This may not be suitable for your use case. Please override `configure_transforms` in your model.
2024-06-12 19:20:19,736 - INFO - Found the dataset and train/test split.
2024-06-12 19:20:20,840 - INFO - Loading pretrained weights from url (https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/wide_resnet50_racm-8234f177.pth)
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/home/jinyao/anaconda3/envs/IADBE/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=63` in the `DataLoader` to improve performance.
┏━━━┳━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃   ┃ Name                  ┃ Type                     ┃ Params ┃
┡━━━╇━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0 │ loss                  │ ReverseDistillationLoss  │      0 │
│ 1 │ _transform            │ Compose                  │      0 │
│ 2 │ normalization_metrics │ MinMax                   │      0 │
│ 3 │ image_threshold       │ F1AdaptiveThreshold      │      0 │
│ 4 │ pixel_threshold       │ F1AdaptiveThreshold      │      0 │
│ 5 │ image_metrics         │ AnomalibMetricCollection │      0 │
│ 6 │ pixel_metrics         │ AnomalibMetricCollection │      0 │
│ 7 │ model                 │ ReverseDistillationModel │ 89.0 M │
└───┴───────────────────────┴──────────────────────────┴────────┘
Trainable params: 89.0 M
Non-trainable params: 0
Total params: 89.0 M
Total estimated model params size (MB): 356
/home/jinyao/anaconda3/envs/IADBE/lib/python3.10/site-packages/lightning/pytorch/core/module.py:494: You called `self.log('train_loss', ..., logger=True)` but have no logger configured. You can enable one by doing `Trainer(logger=ALogger(...))`
/home/jinyao/anaconda3/envs/IADBE/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=63` in the `DataLoader` to improve performance.
`Trainer.fit` stopped: `max_epochs=200` reached.
2024-06-12 19:54:18,466 - INFO - Training took 2036.97 seconds
2024-06-12 19:54:18,467 - INFO - ================== Start testing for dataset: chewinggum ==================
2024-06-12 19:54:18,469 - INFO - Found the dataset and train/test split.
Epoch 199/199 ━━━━━━━━━━━━━━━━ 15/15 0:00:10 •        1.49it/s train_loss_step:
                                     0:00:00                   0.107
                                                               train_loss_epoch:
                                                               0.105
                                                               pixel_AUROC:
                                                               0.988 pixel_PRO:
                                                               0.313
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/home/jinyao/anaconda3/envs/IADBE/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=63` in the `DataLoader` to improve performance.
2024-06-12 19:54:45,486 - INFO - Testing took 26.76963520050049 seconds
Throughput (batch_size=32) : 5.603363619882111 FPS
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃        Test metric        ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│        image_AUROC        │    0.9941999316215515     │
│         image_PRO         │            1.0            │
│        pixel_AUROC        │    0.9881815910339355     │
│         pixel_PRO         │    0.3129956126213074     │
└───────────────────────────┴───────────────────────────┘
Testing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5/5 0:00:20 • 0:00:00 0.19it/s
2024-06-12 19:54:45,693 - INFO - ================== Processing dataset: fryum ==================
2024-06-12 19:54:45,693 - INFO - Initializing ReverseDistillation model.
2024-06-12 19:54:45,694 - INFO - ================== Start training for dataset: fryum ==================
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
2024-06-12 19:54:45,703 - WARNING - No implementation of `configure_transforms` was provided in the Lightning model. Using default transforms from the base class. This may not be suitable for your use case. Please override `configure_transforms` in your model.
2024-06-12 19:54:45,704 - INFO - Found the dataset and train/test split.
2024-06-12 19:54:46,819 - INFO - Loading pretrained weights from url (https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/wide_resnet50_racm-8234f177.pth)
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/home/jinyao/anaconda3/envs/IADBE/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=63` in the `DataLoader` to improve performance.
┏━━━┳━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃   ┃ Name                  ┃ Type                     ┃ Params ┃
┡━━━╇━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0 │ loss                  │ ReverseDistillationLoss  │      0 │
│ 1 │ _transform            │ Compose                  │      0 │
│ 2 │ normalization_metrics │ MinMax                   │      0 │
│ 3 │ image_threshold       │ F1AdaptiveThreshold      │      0 │
│ 4 │ pixel_threshold       │ F1AdaptiveThreshold      │      0 │
│ 5 │ image_metrics         │ AnomalibMetricCollection │      0 │
│ 6 │ pixel_metrics         │ AnomalibMetricCollection │      0 │
│ 7 │ model                 │ ReverseDistillationModel │ 89.0 M │
└───┴───────────────────────┴──────────────────────────┴────────┘
Trainable params: 89.0 M
Non-trainable params: 0
Total params: 89.0 M
Total estimated model params size (MB): 356
/home/jinyao/anaconda3/envs/IADBE/lib/python3.10/site-packages/lightning/pytorch/core/module.py:494: You called `self.log('train_loss', ..., logger=True)` but have no logger configured. You can enable one by doing `Trainer(logger=ALogger(...))`
/home/jinyao/anaconda3/envs/IADBE/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=63` in the `DataLoader` to improve performance.
`Trainer.fit` stopped: `max_epochs=200` reached.
2024-06-12 20:29:13,347 - INFO - Training took 2065.89 seconds
2024-06-12 20:29:13,349 - INFO - ================== Start testing for dataset: fryum ==================
2024-06-12 20:29:13,350 - INFO - Found the dataset and train/test split.
Epoch 199/199 ━━━━━━━━━━━━━━━━ 15/15 0:00:10 •        1.46it/s train_loss_step:
                                     0:00:00                   0.059
                                                               train_loss_epoch:
                                                               0.057
                                                               pixel_AUROC:
                                                               0.984 pixel_PRO:
                                                               0.476
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/home/jinyao/anaconda3/envs/IADBE/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=63` in the `DataLoader` to improve performance.
2024-06-12 20:29:40,694 - INFO - Testing took 27.126426696777344 seconds
Throughput (batch_size=32) : 5.529663072719424 FPS
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃        Test metric        ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│        image_AUROC        │    0.9061999917030334     │
│         image_PRO         │            1.0            │
│        pixel_AUROC        │    0.9618629813194275     │
│         pixel_PRO         │     0.476008802652359     │
└───────────────────────────┴───────────────────────────┘
Testing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5/5 0:00:21 • 0:00:00 0.19it/s
2024-06-12 20:29:40,898 - INFO - ================== Processing dataset: macaroni1 ==================
2024-06-12 20:29:40,898 - INFO - Initializing ReverseDistillation model.
2024-06-12 20:29:40,899 - INFO - ================== Start training for dataset: macaroni1 ==================
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
2024-06-12 20:29:40,909 - WARNING - No implementation of `configure_transforms` was provided in the Lightning model. Using default transforms from the base class. This may not be suitable for your use case. Please override `configure_transforms` in your model.
2024-06-12 20:29:40,909 - INFO - Found the dataset and train/test split.
2024-06-12 20:29:42,023 - INFO - Loading pretrained weights from url (https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/wide_resnet50_racm-8234f177.pth)
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/home/jinyao/anaconda3/envs/IADBE/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=63` in the `DataLoader` to improve performance.
┏━━━┳━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃   ┃ Name                  ┃ Type                     ┃ Params ┃
┡━━━╇━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0 │ loss                  │ ReverseDistillationLoss  │      0 │
│ 1 │ _transform            │ Compose                  │      0 │
│ 2 │ normalization_metrics │ MinMax                   │      0 │
│ 3 │ image_threshold       │ F1AdaptiveThreshold      │      0 │
│ 4 │ pixel_threshold       │ F1AdaptiveThreshold      │      0 │
│ 5 │ image_metrics         │ AnomalibMetricCollection │      0 │
│ 6 │ pixel_metrics         │ AnomalibMetricCollection │      0 │
│ 7 │ model                 │ ReverseDistillationModel │ 89.0 M │
└───┴───────────────────────┴──────────────────────────┴────────┘
Trainable params: 89.0 M
Non-trainable params: 0
Total params: 89.0 M
Total estimated model params size (MB): 356
/home/jinyao/anaconda3/envs/IADBE/lib/python3.10/site-packages/lightning/pytorch/core/module.py:494: You called `self.log('train_loss', ..., logger=True)` but have no logger configured. You can enable one by doing `Trainer(logger=ALogger(...))`
/home/jinyao/anaconda3/envs/IADBE/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=63` in the `DataLoader` to improve performance.
`Trainer.fit` stopped: `max_epochs=200` reached.
2024-06-12 21:39:48,028 - INFO - Training took 4205.36 seconds
2024-06-12 21:39:48,030 - INFO - ================== Start testing for dataset: macaroni1 ==================
2024-06-12 21:39:48,032 - INFO - Found the dataset and train/test split.
Epoch 199/199 ━━━━━━━━━━━━━━━━ 29/29 0:00:21 •        1.36it/s train_loss_step:
                                     0:00:00                   0.050
                                                               train_loss_epoch:
                                                               0.048
                                                               pixel_AUROC:
                                                               0.994 pixel_PRO:
                                                               0.184
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/home/jinyao/anaconda3/envs/IADBE/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=63` in the `DataLoader` to improve performance.
2024-06-12 21:40:23,137 - INFO - Testing took 34.86970615386963 seconds
Throughput (batch_size=32) : 5.735637665469837 FPS
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃        Test metric        ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│        image_AUROC        │    0.9620999693870544     │
│         image_PRO         │            1.0            │
│        pixel_AUROC        │    0.9936097860336304     │
│         pixel_PRO         │    0.1837678998708725     │
└───────────────────────────┴───────────────────────────┘
2024-06-12 21:40:23,342 - INFO - ================== Processing dataset: macaroni2 ==================
2024-06-12 21:40:23,342 - INFO - Initializing ReverseDistillation model.
2024-06-12 21:40:23,342 - INFO - ================== Start training for dataset: macaroni2 ==================
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
Testing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7/7 0:00:30 • 0:00:00 0.20it/s
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
2024-06-12 21:40:23,667 - WARNING - No implementation of `configure_transforms` was provided in the Lightning model. Using default transforms from the base class. This may not be suitable for your use case. Please override `configure_transforms` in your model.
2024-06-12 21:40:23,668 - INFO - Found the dataset and train/test split.
2024-06-12 21:40:24,776 - INFO - Loading pretrained weights from url (https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/wide_resnet50_racm-8234f177.pth)
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
┏━━━┳━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃   ┃ Name                  ┃ Type                     ┃ Params ┃
┡━━━╇━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0 │ loss                  │ ReverseDistillationLoss  │      0 │
│ 1 │ _transform            │ Compose                  │      0 │
│ 2 │ normalization_metrics │ MinMax                   │      0 │
│ 3 │ image_threshold       │ F1AdaptiveThreshold      │      0 │
│ 4 │ pixel_threshold       │ F1AdaptiveThreshold      │      0 │
│ 5 │ image_metrics         │ AnomalibMetricCollection │      0 │
│ 6 │ pixel_metrics         │ AnomalibMetricCollection │      0 │
│ 7 │ model                 │ ReverseDistillationModel │ 89.0 M │
└───┴───────────────────────┴──────────────────────────┴────────┘
Trainable params: 89.0 M
Non-trainable params: 0
Total params: 89.0 M
Total estimated model params size (MB): 356
/home/jinyao/anaconda3/envs/IADBE/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=63` in the `DataLoader` to improve performance.
/home/jinyao/anaconda3/envs/IADBE/lib/python3.10/site-packages/lightning/pytorch/core/module.py:494: You called `self.log('train_loss', ..., logger=True)` but have no logger configured. You can enable one by doing `Trainer(logger=ALogger(...))`
/home/jinyao/anaconda3/envs/IADBE/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=63` in the `DataLoader` to improve performance.
`Trainer.fit` stopped: `max_epochs=200` reached.
2024-06-12 22:45:44,065 - INFO - Training took 3918.63 seconds
2024-06-12 22:45:44,067 - INFO - ================== Start testing for dataset: macaroni2 ==================
2024-06-12 22:45:44,068 - INFO - Found the dataset and train/test split.
Epoch 199/199 ━━━━━━━━━━━━━━━━ 29/29 0:00:19 •        1.49it/s train_loss_step:
                                     0:00:00                   0.060
                                                               train_loss_epoch:
                                                               0.059
                                                               pixel_AUROC:
                                                               0.992 pixel_PRO:
                                                               0.219
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/home/jinyao/anaconda3/envs/IADBE/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=63` in the `DataLoader` to improve performance.
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃        Test metric        ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│        image_AUROC        │    0.8476999402046204     │
│         image_PRO         │            1.0            │
│        pixel_AUROC        │    0.9923373460769653     │
│         pixel_PRO         │    0.21945811808109283    │
└───────────────────────────┴───────────────────────────┘
2024-06-12 22:46:18,641 - INFO - Testing took 34.3538293838501 seconds
Throughput (batch_size=32) : 5.8217672843779384 FPS
Testing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7/7 0:00:30 • 0:00:00 0.20it/s
2024-06-12 22:46:18,850 - INFO - ================== Processing dataset: pcb1 ==================
2024-06-12 22:46:18,851 - INFO - Initializing ReverseDistillation model.
2024-06-12 22:46:18,851 - INFO - ================== Start training for dataset: pcb1 ==================
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
2024-06-12 22:46:18,861 - WARNING - No implementation of `configure_transforms` was provided in the Lightning model. Using default transforms from the base class. This may not be suitable for your use case. Please override `configure_transforms` in your model.
2024-06-12 22:46:18,861 - INFO - Found the dataset and train/test split.
2024-06-12 22:46:19,984 - INFO - Loading pretrained weights from url (https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/wide_resnet50_racm-8234f177.pth)
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/home/jinyao/anaconda3/envs/IADBE/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=63` in the `DataLoader` to improve performance.
┏━━━┳━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃   ┃ Name                  ┃ Type                     ┃ Params ┃
┡━━━╇━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0 │ loss                  │ ReverseDistillationLoss  │      0 │
│ 1 │ _transform            │ Compose                  │      0 │
│ 2 │ normalization_metrics │ MinMax                   │      0 │
│ 3 │ image_threshold       │ F1AdaptiveThreshold      │      0 │
│ 4 │ pixel_threshold       │ F1AdaptiveThreshold      │      0 │
│ 5 │ image_metrics         │ AnomalibMetricCollection │      0 │
│ 6 │ pixel_metrics         │ AnomalibMetricCollection │      0 │
│ 7 │ model                 │ ReverseDistillationModel │ 89.0 M │
└───┴───────────────────────┴──────────────────────────┴────────┘
Trainable params: 89.0 M
Non-trainable params: 0
Total params: 89.0 M
Total estimated model params size (MB): 356
/home/jinyao/anaconda3/envs/IADBE/lib/python3.10/site-packages/lightning/pytorch/core/module.py:494: You called `self.log('train_loss', ..., logger=True)` but have no logger configured. You can enable one by doing `Trainer(logger=ALogger(...))`
/home/jinyao/anaconda3/envs/IADBE/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=63` in the `DataLoader` to improve performance.
`Trainer.fit` stopped: `max_epochs=200` reached.
Epoch 199/199 ━━━━━━━━━━━━━━━━ 29/29 0:00:22 •        1.29it/s train_loss_step:
                                     0:00:00                   0.093
                                                               train_loss_epoch:
                                                               0.090
                                                               pixel_AUROC:
                                                               0.997 pixel_PRO:
                                                               0.335
2024-06-13 00:00:30,397 - INFO - Training took 4449.70 seconds
2024-06-13 00:00:30,399 - INFO - ================== Start testing for dataset: pcb1 ==================
2024-06-13 00:00:30,401 - INFO - Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/home/jinyao/anaconda3/envs/IADBE/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=63` in the `DataLoader` to improve performance.
2024-06-13 00:01:06,183 - INFO - Testing took 35.54993486404419 seconds
Throughput (batch_size=32) : 5.6258893515521855 FPS
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃        Test metric        ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│        image_AUROC        │    0.9578999876976013     │
│         image_PRO         │            1.0            │
│        pixel_AUROC        │    0.9969278573989868     │
│         pixel_PRO         │    0.33467814326286316    │
└───────────────────────────┴───────────────────────────┘
Testing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7/7 0:00:30 • 0:00:00 0.20it/s
2024-06-13 00:01:06,392 - INFO - ================== Processing dataset: pcb2 ==================
2024-06-13 00:01:06,393 - INFO - Initializing ReverseDistillation model.
2024-06-13 00:01:06,393 - INFO - ================== Start training for dataset: pcb2 ==================
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
2024-06-13 00:01:06,421 - WARNING - No implementation of `configure_transforms` was provided in the Lightning model. Using default transforms from the base class. This may not be suitable for your use case. Please override `configure_transforms` in your model.
2024-06-13 00:01:06,421 - INFO - Found the dataset and train/test split.
2024-06-13 00:01:07,537 - INFO - Loading pretrained weights from url (https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/wide_resnet50_racm-8234f177.pth)
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/home/jinyao/anaconda3/envs/IADBE/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=63` in the `DataLoader` to improve performance.
┏━━━┳━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃   ┃ Name                  ┃ Type                     ┃ Params ┃
┡━━━╇━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0 │ loss                  │ ReverseDistillationLoss  │      0 │
│ 1 │ _transform            │ Compose                  │      0 │
│ 2 │ normalization_metrics │ MinMax                   │      0 │
│ 3 │ image_threshold       │ F1AdaptiveThreshold      │      0 │
│ 4 │ pixel_threshold       │ F1AdaptiveThreshold      │      0 │
│ 5 │ image_metrics         │ AnomalibMetricCollection │      0 │
│ 6 │ pixel_metrics         │ AnomalibMetricCollection │      0 │
│ 7 │ model                 │ ReverseDistillationModel │ 89.0 M │
└───┴───────────────────────┴──────────────────────────┴────────┘
Trainable params: 89.0 M
Non-trainable params: 0
Total params: 89.0 M
Total estimated model params size (MB): 356
/home/jinyao/anaconda3/envs/IADBE/lib/python3.10/site-packages/lightning/pytorch/core/module.py:494: You called `self.log('train_loss', ..., logger=True)` but have no logger configured. You can enable one by doing `Trainer(logger=ALogger(...))`
/home/jinyao/anaconda3/envs/IADBE/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=63` in the `DataLoader` to improve performance.
`Trainer.fit` stopped: `max_epochs=200` reached.
2024-06-13 01:11:45,790 - INFO - Training took 4237.59 seconds
2024-06-13 01:11:45,791 - INFO - ================== Start testing for dataset: pcb2 ==================
2024-06-13 01:11:45,793 - INFO - Found the dataset and train/test split.
Epoch 199/199 ━━━━━━━━━━━━━━━━ 29/29 0:00:21 •        1.35it/s train_loss_step:
                                     0:00:00                   0.095
                                                               train_loss_epoch:
                                                               0.091
                                                               pixel_AUROC:
                                                               0.988 pixel_PRO:
                                                               0.236
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/home/jinyao/anaconda3/envs/IADBE/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=63` in the `DataLoader` to improve performance.
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃        Test metric        ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│        image_AUROC        │    0.9702000021934509     │
│         image_PRO         │            1.0            │
│        pixel_AUROC        │    0.9877622723579407     │
│         pixel_PRO         │    0.23615644872188568    │
└───────────────────────────┴───────────────────────────┘
2024-06-13 01:12:21,638 - INFO - Testing took 35.62114858627319 seconds
Throughput (batch_size=32) : 5.614642085883528 FPS
Testing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7/7 0:00:31 • 0:00:00 0.20it/s
2024-06-13 01:12:21,854 - INFO - ================== Processing dataset: pcb3 ==================
2024-06-13 01:12:21,854 - INFO - Initializing ReverseDistillation model.
2024-06-13 01:12:21,855 - INFO - ================== Start training for dataset: pcb3 ==================
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
2024-06-13 01:12:21,865 - WARNING - No implementation of `configure_transforms` was provided in the Lightning model. Using default transforms from the base class. This may not be suitable for your use case. Please override `configure_transforms` in your model.
2024-06-13 01:12:21,865 - INFO - Found the dataset and train/test split.
2024-06-13 01:12:22,971 - INFO - Loading pretrained weights from url (https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/wide_resnet50_racm-8234f177.pth)
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/home/jinyao/anaconda3/envs/IADBE/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=63` in the `DataLoader` to improve performance.
┏━━━┳━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃   ┃ Name                  ┃ Type                     ┃ Params ┃
┡━━━╇━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0 │ loss                  │ ReverseDistillationLoss  │      0 │
│ 1 │ _transform            │ Compose                  │      0 │
│ 2 │ normalization_metrics │ MinMax                   │      0 │
│ 3 │ image_threshold       │ F1AdaptiveThreshold      │      0 │
│ 4 │ pixel_threshold       │ F1AdaptiveThreshold      │      0 │
│ 5 │ image_metrics         │ AnomalibMetricCollection │      0 │
│ 6 │ pixel_metrics         │ AnomalibMetricCollection │      0 │
│ 7 │ model                 │ ReverseDistillationModel │ 89.0 M │
└───┴───────────────────────┴──────────────────────────┴────────┘
Trainable params: 89.0 M
Non-trainable params: 0
Total params: 89.0 M
Total estimated model params size (MB): 356
/home/jinyao/anaconda3/envs/IADBE/lib/python3.10/site-packages/lightning/pytorch/core/module.py:494: You called `self.log('train_loss', ..., logger=True)` but have no logger configured. You can enable one by doing `Trainer(logger=ALogger(...))`
/home/jinyao/anaconda3/envs/IADBE/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=63` in the `DataLoader` to improve performance.
`Trainer.fit` stopped: `max_epochs=200` reached.
Epoch 199/199 ━━━━━━━━━━━━━━━━ 29/29 0:00:20 •        1.38it/s train_loss_step:
                                     0:00:00                   0.073
                                                               train_loss_epoch:
                                                               0.074
                                                               pixel_AUROC:
                                                               0.920 pixel_PRO:
                                                               0.074
2024-06-13 02:22:29,051 - INFO - Training took 4205.44 seconds
2024-06-13 02:22:29,053 - INFO - ================== Start testing for dataset: pcb3 ==================
2024-06-13 02:22:29,054 - INFO - Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/home/jinyao/anaconda3/envs/IADBE/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=63` in the `DataLoader` to improve performance.
2024-06-13 02:23:03,884 - INFO - Testing took 34.61161494255066 seconds
Throughput (batch_size=32) : 5.807299091175766 FPS
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃        Test metric        ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│        image_AUROC        │    0.9651485681533813     │
│         image_PRO         │            1.0            │
│        pixel_AUROC        │    0.9918025732040405     │
│         pixel_PRO         │    0.07449454069137573    │
└───────────────────────────┴───────────────────────────┘
Testing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7/7 0:00:30 • 0:00:00 0.20it/s
2024-06-13 02:23:04,097 - INFO - ================== Processing dataset: pcb4 ==================
2024-06-13 02:23:04,097 - INFO - Initializing ReverseDistillation model.
2024-06-13 02:23:04,097 - INFO - ================== Start training for dataset: pcb4 ==================
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
2024-06-13 02:23:04,107 - WARNING - No implementation of `configure_transforms` was provided in the Lightning model. Using default transforms from the base class. This may not be suitable for your use case. Please override `configure_transforms` in your model.
2024-06-13 02:23:04,107 - INFO - Found the dataset and train/test split.
2024-06-13 02:23:05,225 - INFO - Loading pretrained weights from url (https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/wide_resnet50_racm-8234f177.pth)
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
┏━━━┳━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃   ┃ Name                  ┃ Type                     ┃ Params ┃
┡━━━╇━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0 │ loss                  │ ReverseDistillationLoss  │      0 │
│ 1 │ _transform            │ Compose                  │      0 │
│ 2 │ normalization_metrics │ MinMax                   │      0 │
│ 3 │ image_threshold       │ F1AdaptiveThreshold      │      0 │
│ 4 │ pixel_threshold       │ F1AdaptiveThreshold      │      0 │
│ 5 │ image_metrics         │ AnomalibMetricCollection │      0 │
│ 6 │ pixel_metrics         │ AnomalibMetricCollection │      0 │
│ 7 │ model                 │ ReverseDistillationModel │ 89.0 M │
└───┴───────────────────────┴──────────────────────────┴────────┘
Trainable params: 89.0 M
Non-trainable params: 0
Total params: 89.0 M
Total estimated model params size (MB): 356
/home/jinyao/anaconda3/envs/IADBE/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=63` in the `DataLoader` to improve performance.
/home/jinyao/anaconda3/envs/IADBE/lib/python3.10/site-packages/lightning/pytorch/core/module.py:494: You called `self.log('train_loss', ..., logger=True)` but have no logger configured. You can enable one by doing `Trainer(logger=ALogger(...))`
/home/jinyao/anaconda3/envs/IADBE/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=63` in the `DataLoader` to improve performance.
`Trainer.fit` stopped: `max_epochs=200` reached.
2024-06-13 03:34:56,742 - INFO - Training took 4310.87 seconds
2024-06-13 03:34:56,744 - INFO - ================== Start testing for dataset: pcb4 ==================
2024-06-13 03:34:56,746 - INFO - Found the dataset and train/test split.
Epoch 199/199 ━━━━━━━━━━━━━━━━ 29/29 0:00:21 •        1.36it/s train_loss_step:
                                     0:00:00                   0.085
                                                               train_loss_epoch:
                                                               0.085
                                                               pixel_AUROC:
                                                               0.983 pixel_PRO:
                                                               0.206
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/home/jinyao/anaconda3/envs/IADBE/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=63` in the `DataLoader` to improve performance.
2024-06-13 03:35:32,540 - INFO - Testing took 35.57167673110962 seconds
Throughput (batch_size=32) : 5.650562989183277 FPS
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃        Test metric        ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│        image_AUROC        │    0.9989108443260193     │
│         image_PRO         │            1.0            │
│        pixel_AUROC        │    0.9828081130981445     │
│         pixel_PRO         │    0.20642435550689697    │
└───────────────────────────┴───────────────────────────┘
Testing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7/7 0:00:30 • 0:00:00 0.20it/s
2024-06-13 03:35:32,750 - INFO - ================== Processing dataset: pipe_fryum ==================
2024-06-13 03:35:32,750 - INFO - Initializing ReverseDistillation model.
2024-06-13 03:35:32,751 - INFO - ================== Start training for dataset: pipe_fryum ==================
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
2024-06-13 03:35:32,762 - WARNING - No implementation of `configure_transforms` was provided in the Lightning model. Using default transforms from the base class. This may not be suitable for your use case. Please override `configure_transforms` in your model.
2024-06-13 03:35:32,762 - INFO - Found the dataset and train/test split.
2024-06-13 03:35:33,844 - INFO - Loading pretrained weights from url (https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/wide_resnet50_racm-8234f177.pth)
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
┏━━━┳━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃   ┃ Name                  ┃ Type                     ┃ Params ┃
┡━━━╇━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0 │ loss                  │ ReverseDistillationLoss  │      0 │
│ 1 │ _transform            │ Compose                  │      0 │
│ 2 │ normalization_metrics │ MinMax                   │      0 │
│ 3 │ image_threshold       │ F1AdaptiveThreshold      │      0 │
│ 4 │ pixel_threshold       │ F1AdaptiveThreshold      │      0 │
│ 5 │ image_metrics         │ AnomalibMetricCollection │      0 │
│ 6 │ pixel_metrics         │ AnomalibMetricCollection │      0 │
│ 7 │ model                 │ ReverseDistillationModel │ 89.0 M │
└───┴───────────────────────┴──────────────────────────┴────────┘
Trainable params: 89.0 M
Non-trainable params: 0
Total params: 89.0 M
Total estimated model params size (MB): 356
/home/jinyao/anaconda3/envs/IADBE/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=63` in the `DataLoader` to improve performance.
/home/jinyao/anaconda3/envs/IADBE/lib/python3.10/site-packages/lightning/pytorch/core/module.py:494: You called `self.log('train_loss', ..., logger=True)` but have no logger configured. You can enable one by doing `Trainer(logger=ALogger(...))`
/home/jinyao/anaconda3/envs/IADBE/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=63` in the `DataLoader` to improve performance.
`Trainer.fit` stopped: `max_epochs=200` reached.
2024-06-13 04:08:19,795 - INFO - Training took 1965.30 seconds
2024-06-13 04:08:19,797 - INFO - ================== Start testing for dataset: pipe_fryum ==================
2024-06-13 04:08:19,798 - INFO - Found the dataset and train/test split.
Epoch 199/199 ━━━━━━━━━━━━━━━━ 15/15 0:00:09 •        1.53it/s train_loss_step:
                                     0:00:00                   0.075
                                                               train_loss_epoch:
                                                               0.071
                                                               pixel_AUROC:
                                                               0.929 pixel_PRO:
                                                               0.573
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/home/jinyao/anaconda3/envs/IADBE/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=63` in the `DataLoader` to improve performance.
2024-06-13 04:08:46,713 - INFO - Testing took 26.697558164596558 seconds
Throughput (batch_size=32) : 5.618491364461711 FPS
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃        Test metric        ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│        image_AUROC        │    0.9846000075340271     │
│         image_PRO         │            1.0            │
│        pixel_AUROC        │    0.9896143078804016     │
│         pixel_PRO         │     0.573219358921051     │
└───────────────────────────┴───────────────────────────┘
Testing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5/5 0:00:20 • 0:00:00 0.19it/s
