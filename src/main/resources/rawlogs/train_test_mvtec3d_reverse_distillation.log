/home/jinyao/anaconda3/envs/IADBE/bin/python /home/jinyao/PycharmProjects/IADBE/train_test_mvtec3d_reverse_distillation.py
2024-06-06 11:16:45,105 - INFO - ================== Processing dataset: bagel ==================
2024-06-06 11:16:45,105 - INFO - Initializing ReverseDistillation model.
2024-06-06 11:16:45,106 - INFO - ================== Start training for dataset: bagel ==================
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
2024-06-06 11:16:45,192 - WARNING - No implementation of `configure_transforms` was provided in the Lightning model. Using default transforms from the base class. This may not be suitable for your use case. Please override `configure_transforms` in your model.
2024-06-06 11:16:45,192 - INFO - Found the dataset.
You are using a CUDA device ('NVIDIA GeForce RTX 3090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
2024-06-06 11:16:46,473 - INFO - Loading pretrained weights from url (https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/wide_resnet50_racm-8234f177.pth)
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/home/jinyao/anaconda3/envs/IADBE/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=63` in the `DataLoader` to improve performance.
┏━━━┳━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃   ┃ Name                  ┃ Type                     ┃ Params ┃
┡━━━╇━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0 │ loss                  │ ReverseDistillationLoss  │      0 │
│ 1 │ _transform            │ Compose                  │      0 │
│ 2 │ normalization_metrics │ MinMax                   │      0 │
│ 3 │ image_threshold       │ F1AdaptiveThreshold      │      0 │
│ 4 │ pixel_threshold       │ F1AdaptiveThreshold      │      0 │
│ 5 │ image_metrics         │ AnomalibMetricCollection │      0 │
│ 6 │ pixel_metrics         │ AnomalibMetricCollection │      0 │
│ 7 │ model                 │ ReverseDistillationModel │ 89.0 M │
└───┴───────────────────────┴──────────────────────────┴────────┘
Trainable params: 89.0 M
Non-trainable params: 0
Total params: 89.0 M
Total estimated model params size (MB): 356
/home/jinyao/anaconda3/envs/IADBE/lib/python3.10/site-packages/lightning/pytorch/core/module.py:494: You called `self.log('train_loss', ..., logger=True)` but have no logger configured. You can enable one by doing `Trainer(logger=ALogger(...))`
/home/jinyao/anaconda3/envs/IADBE/lib/python3.10/site-packages/lightning/pytorch/utilities/data.py:77: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 32. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.
/home/jinyao/anaconda3/envs/IADBE/lib/python3.10/site-packages/lightning/pytorch/utilities/data.py:77: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 20. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.
/home/jinyao/anaconda3/envs/IADBE/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=63` in the `DataLoader` to improve performance.
`Trainer.fit` stopped: `max_epochs=200` reached.
Epoch 199/199 ━━━━━━━━━━━━━━━━━ 8/8 0:00:08 • 0:00:00 0.98it/s train_loss_step:
                                                               0.095
                                                               train_loss_epoch:
                                                               0.095
                                                               pixel_AUROC:
                                                               0.907 pixel_PRO:
                                                               0.388
2024-06-06 11:44:03,829 - INFO - Training took 1636.54 seconds
2024-06-06 11:44:03,830 - INFO - ================== Start testing for dataset: bagel ==================
2024-06-06 11:44:03,832 - INFO - Found the dataset.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/home/jinyao/anaconda3/envs/IADBE/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=63` in the `DataLoader` to improve performance.
2024-06-06 11:44:24,523 - INFO - Testing took 20.43094778060913 seconds
Throughput (batch_size=32) : 5.3839890924884175 FPS
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃        Test metric        ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│        image_AUROC        │     0.961776852607727     │
│         image_PRO         │            1.0            │
│        pixel_AUROC        │    0.9817235469818115     │
│         pixel_PRO         │    0.3878036439418793     │
└───────────────────────────┴───────────────────────────┘
2024-06-06 11:44:24,795 - INFO - ================== Processing dataset: cable_gland ==================
2024-06-06 11:44:24,795 - INFO - Initializing ReverseDistillation model.
2024-06-06 11:44:24,796 - INFO - ================== Start training for dataset: cable_gland ==================
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
2024-06-06 11:44:24,805 - WARNING - No implementation of `configure_transforms` was provided in the Lightning model. Using default transforms from the base class. This may not be suitable for your use case. Please override `configure_transforms` in your model.
2024-06-06 11:44:24,805 - INFO - Found the dataset.
Testing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4/4 0:00:16 • 0:00:00 0.19it/s
2024-06-06 11:44:26,043 - INFO - Loading pretrained weights from url (https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/wide_resnet50_racm-8234f177.pth)
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/home/jinyao/anaconda3/envs/IADBE/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=63` in the `DataLoader` to improve performance.
┏━━━┳━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃   ┃ Name                  ┃ Type                     ┃ Params ┃
┡━━━╇━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0 │ loss                  │ ReverseDistillationLoss  │      0 │
│ 1 │ _transform            │ Compose                  │      0 │
│ 2 │ normalization_metrics │ MinMax                   │      0 │
│ 3 │ image_threshold       │ F1AdaptiveThreshold      │      0 │
│ 4 │ pixel_threshold       │ F1AdaptiveThreshold      │      0 │
│ 5 │ image_metrics         │ AnomalibMetricCollection │      0 │
│ 6 │ pixel_metrics         │ AnomalibMetricCollection │      0 │
│ 7 │ model                 │ ReverseDistillationModel │ 89.0 M │
└───┴───────────────────────┴──────────────────────────┴────────┘
Trainable params: 89.0 M
Non-trainable params: 0
Total params: 89.0 M
Total estimated model params size (MB): 356
/home/jinyao/anaconda3/envs/IADBE/lib/python3.10/site-packages/lightning/pytorch/core/module.py:494: You called `self.log('train_loss', ..., logger=True)` but have no logger configured. You can enable one by doing `Trainer(logger=ALogger(...))`
/home/jinyao/anaconda3/envs/IADBE/lib/python3.10/site-packages/lightning/pytorch/utilities/data.py:77: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 31. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.
/home/jinyao/anaconda3/envs/IADBE/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=63` in the `DataLoader` to improve performance.
`Trainer.fit` stopped: `max_epochs=200` reached.
2024-06-06 11:55:13,336 - INFO - Training took 646.40 seconds
2024-06-06 11:55:13,337 - INFO - ================== Start testing for dataset: cable_gland ==================
2024-06-06 11:55:13,339 - INFO - Found the dataset.
Epoch 199/199 ━━━━━━━━━━━━━━━━━ 7/7 0:00:03 • 0:00:00 2.24it/s train_loss_step:
                                                               0.086
                                                               train_loss_epoch:
                                                               0.085
                                                               pixel_AUROC:
                                                               0.991 pixel_PRO:
                                                               0.397
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/home/jinyao/anaconda3/envs/IADBE/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=63` in the `DataLoader` to improve performance.
2024-06-06 11:55:29,140 - INFO - Testing took 15.595863580703735 seconds
Throughput (batch_size=32) : 6.924913098985102 FPS
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃        Test metric        ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│        image_AUROC        │    0.9074985980987549     │
│         image_PRO         │            1.0            │
│        pixel_AUROC        │    0.9906361103057861     │
│         pixel_PRO         │    0.39681825041770935    │
└───────────────────────────┴───────────────────────────┘
Testing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4/4 0:00:12 • 0:00:00 0.24it/s
2024-06-06 11:55:29,365 - INFO - ================== Processing dataset: carrot ==================
2024-06-06 11:55:29,366 - INFO - Initializing ReverseDistillation model.
2024-06-06 11:55:29,366 - INFO - ================== Start training for dataset: carrot ==================
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
2024-06-06 11:55:29,377 - WARNING - No implementation of `configure_transforms` was provided in the Lightning model. Using default transforms from the base class. This may not be suitable for your use case. Please override `configure_transforms` in your model.
2024-06-06 11:55:29,377 - INFO - Found the dataset.
2024-06-06 11:55:30,595 - INFO - Loading pretrained weights from url (https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/wide_resnet50_racm-8234f177.pth)
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/home/jinyao/anaconda3/envs/IADBE/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=63` in the `DataLoader` to improve performance.
┏━━━┳━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃   ┃ Name                  ┃ Type                     ┃ Params ┃
┡━━━╇━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0 │ loss                  │ ReverseDistillationLoss  │      0 │
│ 1 │ _transform            │ Compose                  │      0 │
│ 2 │ normalization_metrics │ MinMax                   │      0 │
│ 3 │ image_threshold       │ F1AdaptiveThreshold      │      0 │
│ 4 │ pixel_threshold       │ F1AdaptiveThreshold      │      0 │
│ 5 │ image_metrics         │ AnomalibMetricCollection │      0 │
│ 6 │ pixel_metrics         │ AnomalibMetricCollection │      0 │
│ 7 │ model                 │ ReverseDistillationModel │ 89.0 M │
└───┴───────────────────────┴──────────────────────────┴────────┘
Trainable params: 89.0 M
Non-trainable params: 0
Total params: 89.0 M
Total estimated model params size (MB): 356
/home/jinyao/anaconda3/envs/IADBE/lib/python3.10/site-packages/lightning/pytorch/core/module.py:494: You called `self.log('train_loss', ..., logger=True)` but have no logger configured. You can enable one by doing `Trainer(logger=ALogger(...))`
/home/jinyao/anaconda3/envs/IADBE/lib/python3.10/site-packages/lightning/pytorch/utilities/data.py:77: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 30. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.
/home/jinyao/anaconda3/envs/IADBE/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=63` in the `DataLoader` to improve performance.
`Trainer.fit` stopped: `max_epochs=200` reached.
2024-06-06 12:25:48,241 - INFO - Training took 1816.90 seconds
2024-06-06 12:25:48,243 - INFO - ================== Start testing for dataset: carrot ==================
2024-06-06 12:25:48,245 - INFO - Found the dataset.
Epoch 199/199 ━━━━━━━━━━━━━━━━━ 9/9 0:00:09 • 0:00:00 0.98it/s train_loss_step:
                                                               0.063
                                                               train_loss_epoch:
                                                               0.061
                                                               pixel_AUROC:
                                                               0.994 pixel_PRO:
                                                               0.181
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/home/jinyao/anaconda3/envs/IADBE/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=63` in the `DataLoader` to improve performance.
2024-06-06 12:26:15,835 - INFO - Testing took 27.372310876846313 seconds
Throughput (batch_size=32) : 5.8087897918218845 FPS
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃        Test metric        ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│        image_AUROC        │    0.9144219756126404     │
│         image_PRO         │            1.0            │
│        pixel_AUROC        │    0.9926289916038513     │
│         pixel_PRO         │    0.18113842606544495    │
└───────────────────────────┴───────────────────────────┘
Testing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5/5 0:00:21 • 0:00:00 0.19it/s
2024-06-06 12:26:16,056 - INFO - ================== Processing dataset: cookie ==================
2024-06-06 12:26:16,056 - INFO - Initializing ReverseDistillation model.
2024-06-06 12:26:16,057 - INFO - ================== Start training for dataset: cookie ==================
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
2024-06-06 12:26:16,068 - WARNING - No implementation of `configure_transforms` was provided in the Lightning model. Using default transforms from the base class. This may not be suitable for your use case. Please override `configure_transforms` in your model.
2024-06-06 12:26:16,069 - INFO - Found the dataset.
2024-06-06 12:26:17,257 - INFO - Loading pretrained weights from url (https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/wide_resnet50_racm-8234f177.pth)
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/home/jinyao/anaconda3/envs/IADBE/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=63` in the `DataLoader` to improve performance.
┏━━━┳━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃   ┃ Name                  ┃ Type                     ┃ Params ┃
┡━━━╇━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0 │ loss                  │ ReverseDistillationLoss  │      0 │
│ 1 │ _transform            │ Compose                  │      0 │
│ 2 │ normalization_metrics │ MinMax                   │      0 │
│ 3 │ image_threshold       │ F1AdaptiveThreshold      │      0 │
│ 4 │ pixel_threshold       │ F1AdaptiveThreshold      │      0 │
│ 5 │ image_metrics         │ AnomalibMetricCollection │      0 │
│ 6 │ pixel_metrics         │ AnomalibMetricCollection │      0 │
│ 7 │ model                 │ ReverseDistillationModel │ 89.0 M │
└───┴───────────────────────┴──────────────────────────┴────────┘
Trainable params: 89.0 M
Non-trainable params: 0
Total params: 89.0 M
Total estimated model params size (MB): 356
/home/jinyao/anaconda3/envs/IADBE/lib/python3.10/site-packages/lightning/pytorch/core/module.py:494: You called `self.log('train_loss', ..., logger=True)` but have no logger configured. You can enable one by doing `Trainer(logger=ALogger(...))`
/home/jinyao/anaconda3/envs/IADBE/lib/python3.10/site-packages/lightning/pytorch/utilities/data.py:77: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 18. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.
/home/jinyao/anaconda3/envs/IADBE/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=63` in the `DataLoader` to improve performance.
`Trainer.fit` stopped: `max_epochs=200` reached.
2024-06-06 12:38:47,779 - INFO - Training took 749.83 seconds
2024-06-06 12:38:47,781 - INFO - ================== Start testing for dataset: cookie ==================
2024-06-06 12:38:47,782 - INFO - Found the dataset.
Epoch 199/199 ━━━━━━━━━━━━━━━━━ 7/7 0:00:03 • 0:00:00 1.89it/s train_loss_step:
                                                               0.106
                                                               train_loss_epoch:
                                                               0.107
                                                               pixel_AUROC:
                                                               0.968 pixel_PRO:
                                                               0.349
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/home/jinyao/anaconda3/envs/IADBE/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=63` in the `DataLoader` to improve performance.
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃        Test metric        ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│        image_AUROC        │    0.6640083193778992     │
│         image_PRO         │            1.0            │
│        pixel_AUROC        │    0.9683201313018799     │
│         pixel_PRO         │    0.3488142192363739     │
└───────────────────────────┴───────────────────────────┘
2024-06-06 12:39:08,486 - INFO - Testing took 20.496733903884888 seconds
Throughput (batch_size=32) : 6.391262169587451 FPS
Testing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5/5 0:00:17 • 0:00:00 0.23it/s
2024-06-06 12:39:08,702 - INFO - ================== Processing dataset: dowel ==================
2024-06-06 12:39:08,702 - INFO - Initializing ReverseDistillation model.
2024-06-06 12:39:08,703 - INFO - ================== Start training for dataset: dowel ==================
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
2024-06-06 12:39:08,713 - WARNING - No implementation of `configure_transforms` was provided in the Lightning model. Using default transforms from the base class. This may not be suitable for your use case. Please override `configure_transforms` in your model.
2024-06-06 12:39:08,714 - INFO - Found the dataset.
2024-06-06 12:39:09,889 - INFO - Loading pretrained weights from url (https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/wide_resnet50_racm-8234f177.pth)
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/home/jinyao/anaconda3/envs/IADBE/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=63` in the `DataLoader` to improve performance.
┏━━━┳━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃   ┃ Name                  ┃ Type                     ┃ Params ┃
┡━━━╇━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0 │ loss                  │ ReverseDistillationLoss  │      0 │
│ 1 │ _transform            │ Compose                  │      0 │
│ 2 │ normalization_metrics │ MinMax                   │      0 │
│ 3 │ image_threshold       │ F1AdaptiveThreshold      │      0 │
│ 4 │ pixel_threshold       │ F1AdaptiveThreshold      │      0 │
│ 5 │ image_metrics         │ AnomalibMetricCollection │      0 │
│ 6 │ pixel_metrics         │ AnomalibMetricCollection │      0 │
│ 7 │ model                 │ ReverseDistillationModel │ 89.0 M │
└───┴───────────────────────┴──────────────────────────┴────────┘
Trainable params: 89.0 M
Non-trainable params: 0
Total params: 89.0 M
Total estimated model params size (MB): 356
/home/jinyao/anaconda3/envs/IADBE/lib/python3.10/site-packages/lightning/pytorch/core/module.py:494: You called `self.log('train_loss', ..., logger=True)` but have no logger configured. You can enable one by doing `Trainer(logger=ALogger(...))`
/home/jinyao/anaconda3/envs/IADBE/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=63` in the `DataLoader` to improve performance.
`Trainer.fit` stopped: `max_epochs=200` reached.
2024-06-06 12:52:38,170 - INFO - Training took 807.52 seconds
2024-06-06 12:52:38,177 - INFO - ================== Start testing for dataset: dowel ==================
2024-06-06 12:52:38,181 - INFO - Found the dataset.
Epoch 199/199 ━━━━━━━━━━━━━━━━━ 9/9 0:00:03 • 0:00:00 2.27it/s train_loss_step:
                                                               0.073
                                                               train_loss_epoch:
                                                               0.073
                                                               pixel_AUROC:
                                                               0.998 pixel_PRO:
                                                               0.505
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/home/jinyao/anaconda3/envs/IADBE/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=63` in the `DataLoader` to improve performance.
2024-06-06 12:52:57,279 - INFO - Testing took 18.856623649597168 seconds
Throughput (batch_size=32) : 6.894129215055802 FPS
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃        Test metric        ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│        image_AUROC        │    0.9933432936668396     │
│         image_PRO         │            1.0            │
│        pixel_AUROC        │    0.9980435967445374     │
│         pixel_PRO         │    0.5054079294204712     │
└───────────────────────────┴───────────────────────────┘
Testing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5/5 0:00:17 • 0:00:00 0.24it/s
2024-06-06 12:52:57,480 - INFO - ================== Processing dataset: foam ==================
2024-06-06 12:52:57,481 - INFO - Initializing ReverseDistillation model.
2024-06-06 12:52:57,481 - INFO - ================== Start training for dataset: foam ==================
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
2024-06-06 12:52:57,490 - WARNING - No implementation of `configure_transforms` was provided in the Lightning model. Using default transforms from the base class. This may not be suitable for your use case. Please override `configure_transforms` in your model.
2024-06-06 12:52:57,491 - INFO - Found the dataset.
2024-06-06 12:52:58,679 - INFO - Loading pretrained weights from url (https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/wide_resnet50_racm-8234f177.pth)
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/home/jinyao/anaconda3/envs/IADBE/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=63` in the `DataLoader` to improve performance.
┏━━━┳━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃   ┃ Name                  ┃ Type                     ┃ Params ┃
┡━━━╇━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0 │ loss                  │ ReverseDistillationLoss  │      0 │
│ 1 │ _transform            │ Compose                  │      0 │
│ 2 │ normalization_metrics │ MinMax                   │      0 │
│ 3 │ image_threshold       │ F1AdaptiveThreshold      │      0 │
│ 4 │ pixel_threshold       │ F1AdaptiveThreshold      │      0 │
│ 5 │ image_metrics         │ AnomalibMetricCollection │      0 │
│ 6 │ pixel_metrics         │ AnomalibMetricCollection │      0 │
│ 7 │ model                 │ ReverseDistillationModel │ 89.0 M │
└───┴───────────────────────┴──────────────────────────┴────────┘
Trainable params: 89.0 M
Non-trainable params: 0
Total params: 89.0 M
Total estimated model params size (MB): 356
/home/jinyao/anaconda3/envs/IADBE/lib/python3.10/site-packages/lightning/pytorch/core/module.py:494: You called `self.log('train_loss', ..., logger=True)` but have no logger configured. You can enable one by doing `Trainer(logger=ALogger(...))`
/home/jinyao/anaconda3/envs/IADBE/lib/python3.10/site-packages/lightning/pytorch/utilities/data.py:77: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 12. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.
/home/jinyao/anaconda3/envs/IADBE/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=63` in the `DataLoader` to improve performance.
`Trainer.fit` stopped: `max_epochs=200` reached.
2024-06-06 13:22:36,774 - INFO - Training took 1777.38 seconds
2024-06-06 13:22:36,776 - INFO - ================== Start testing for dataset: foam ==================
2024-06-06 13:22:36,777 - INFO - Found the dataset.
Epoch 199/199 ━━━━━━━━━━━━━━━━━ 8/8 0:00:08 • 0:00:00 0.92it/s train_loss_step:
                                                               0.092
                                                               train_loss_epoch:
                                                               0.092
                                                               pixel_AUROC:
                                                               0.999 pixel_PRO:
                                                               0.495
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/home/jinyao/anaconda3/envs/IADBE/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=63` in the `DataLoader` to improve performance.
2024-06-06 13:22:56,117 - INFO - Testing took 19.111191987991333 seconds
Throughput (batch_size=32) : 5.232535995809983 FPS
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃        Test metric        ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│        image_AUROC        │    0.8356250524520874     │
│         image_PRO         │            1.0            │
│        pixel_AUROC        │    0.9986295700073242     │
│         pixel_PRO         │    0.4948306679725647     │
└───────────────────────────┴───────────────────────────┘
2024-06-06 13:22:56,322 - INFO - ================== Processing dataset: peach ==================
2024-06-06 13:22:56,323 - INFO - Initializing ReverseDistillation model.
2024-06-06 13:22:56,323 - INFO - ================== Start training for dataset: peach ==================
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
2024-06-06 13:22:56,332 - WARNING - No implementation of `configure_transforms` was provided in the Lightning model. Using default transforms from the base class. This may not be suitable for your use case. Please override `configure_transforms` in your model.
2024-06-06 13:22:56,333 - INFO - Found the dataset.
Testing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4/4 0:00:16 • 0:00:00 0.19it/s
2024-06-06 13:22:57,517 - INFO - Loading pretrained weights from url (https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/wide_resnet50_racm-8234f177.pth)
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
┏━━━┳━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃   ┃ Name                  ┃ Type                     ┃ Params ┃
┡━━━╇━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0 │ loss                  │ ReverseDistillationLoss  │      0 │
│ 1 │ _transform            │ Compose                  │      0 │
│ 2 │ normalization_metrics │ MinMax                   │      0 │
│ 3 │ image_threshold       │ F1AdaptiveThreshold      │      0 │
│ 4 │ pixel_threshold       │ F1AdaptiveThreshold      │      0 │
│ 5 │ image_metrics         │ AnomalibMetricCollection │      0 │
│ 6 │ pixel_metrics         │ AnomalibMetricCollection │      0 │
│ 7 │ model                 │ ReverseDistillationModel │ 89.0 M │
└───┴───────────────────────┴──────────────────────────┴────────┘
Trainable params: 89.0 M
Non-trainable params: 0
Total params: 89.0 M
Total estimated model params size (MB): 356
/home/jinyao/anaconda3/envs/IADBE/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=63` in the `DataLoader` to improve performance.
/home/jinyao/anaconda3/envs/IADBE/lib/python3.10/site-packages/lightning/pytorch/core/module.py:494: You called `self.log('train_loss', ..., logger=True)` but have no logger configured. You can enable one by doing `Trainer(logger=ALogger(...))`
/home/jinyao/anaconda3/envs/IADBE/lib/python3.10/site-packages/lightning/pytorch/utilities/data.py:77: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 9. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.
/home/jinyao/anaconda3/envs/IADBE/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=63` in the `DataLoader` to improve performance.
`Trainer.fit` stopped: `max_epochs=200` reached.
Epoch 199/199 ━━━━━━━━━━━━━━━━ 12/12 0:00:07 •        1.58it/s train_loss_step:
                                     0:00:00                   0.076
                                                               train_loss_epoch:
                                                               0.076
                                                               pixel_AUROC:
                                                               0.993 pixel_PRO:
                                                               0.306
2024-06-06 13:48:27,844 - INFO - Training took 1529.62 seconds
2024-06-06 13:48:27,846 - INFO - ================== Start testing for dataset: peach ==================
2024-06-06 13:48:27,847 - INFO - Found the dataset.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/home/jinyao/anaconda3/envs/IADBE/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=63` in the `DataLoader` to improve performance.
2024-06-06 13:48:48,843 - INFO - Testing took 20.784347772598267 seconds
Throughput (batch_size=32) : 6.350932992664151 FPS
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃        Test metric        ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│        image_AUROC        │    0.8443396091461182     │
│         image_PRO         │            1.0            │
│        pixel_AUROC        │    0.9928607940673828     │
│         pixel_PRO         │    0.30584096908569336    │
└───────────────────────────┴───────────────────────────┘
Testing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5/5 0:00:18 • 0:00:00 0.22it/s
2024-06-06 13:48:49,072 - INFO - ================== Processing dataset: potato ==================
2024-06-06 13:48:49,072 - INFO - Initializing ReverseDistillation model.
2024-06-06 13:48:49,072 - INFO - ================== Start training for dataset: potato ==================
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
2024-06-06 13:48:49,082 - WARNING - No implementation of `configure_transforms` was provided in the Lightning model. Using default transforms from the base class. This may not be suitable for your use case. Please override `configure_transforms` in your model.
2024-06-06 13:48:49,082 - INFO - Found the dataset.
2024-06-06 13:48:50,267 - INFO - Loading pretrained weights from url (https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/wide_resnet50_racm-8234f177.pth)
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
┏━━━┳━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃   ┃ Name                  ┃ Type                     ┃ Params ┃
┡━━━╇━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0 │ loss                  │ ReverseDistillationLoss  │      0 │
│ 1 │ _transform            │ Compose                  │      0 │
│ 2 │ normalization_metrics │ MinMax                   │      0 │
│ 3 │ image_threshold       │ F1AdaptiveThreshold      │      0 │
│ 4 │ pixel_threshold       │ F1AdaptiveThreshold      │      0 │
│ 5 │ image_metrics         │ AnomalibMetricCollection │      0 │
│ 6 │ pixel_metrics         │ AnomalibMetricCollection │      0 │
│ 7 │ model                 │ ReverseDistillationModel │ 89.0 M │
└───┴───────────────────────┴──────────────────────────┴────────┘
Trainable params: 89.0 M
Non-trainable params: 0
Total params: 89.0 M
Total estimated model params size (MB): 356
/home/jinyao/anaconda3/envs/IADBE/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=63` in the `DataLoader` to improve performance.
/home/jinyao/anaconda3/envs/IADBE/lib/python3.10/site-packages/lightning/pytorch/core/module.py:494: You called `self.log('train_loss', ..., logger=True)` but have no logger configured. You can enable one by doing `Trainer(logger=ALogger(...))`
/home/jinyao/anaconda3/envs/IADBE/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=63` in the `DataLoader` to improve performance.
`Trainer.fit` stopped: `max_epochs=200` reached.
2024-06-06 14:20:04,825 - INFO - Training took 1873.84 seconds
2024-06-06 14:20:04,826 - INFO - ================== Start testing for dataset: potato ==================
2024-06-06 14:20:04,828 - INFO - Found the dataset.
Epoch 199/199 ━━━━━━━━━━━━━━━━ 10/10 0:00:09 •        1.08it/s train_loss_step:
                                     0:00:00                   0.057
                                                               train_loss_epoch:
                                                               0.058
                                                               pixel_AUROC:
                                                               0.993 pixel_PRO:
                                                               0.217
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/home/jinyao/anaconda3/envs/IADBE/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=63` in the `DataLoader` to improve performance.
2024-06-06 14:20:24,633 - INFO - Testing took 19.58642864227295 seconds
Throughput (batch_size=32) : 5.820356639900975 FPS
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃        Test metric        ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│        image_AUROC        │    0.6837944984436035     │
│         image_PRO         │            1.0            │
│        pixel_AUROC        │     0.992825448513031     │
│         pixel_PRO         │    0.21720242500305176    │
└───────────────────────────┴───────────────────────────┘
Testing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4/4 0:00:15 • 0:00:00 0.19it/s
2024-06-06 14:20:24,839 - INFO - ================== Processing dataset: rope ==================
2024-06-06 14:20:24,839 - INFO - Initializing ReverseDistillation model.
2024-06-06 14:20:24,840 - INFO - ================== Start training for dataset: rope ==================
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
2024-06-06 14:20:24,850 - WARNING - No implementation of `configure_transforms` was provided in the Lightning model. Using default transforms from the base class. This may not be suitable for your use case. Please override `configure_transforms` in your model.
2024-06-06 14:20:24,851 - INFO - Found the dataset.
2024-06-06 14:20:26,062 - INFO - Loading pretrained weights from url (https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/wide_resnet50_racm-8234f177.pth)
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
┏━━━┳━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃   ┃ Name                  ┃ Type                     ┃ Params ┃
┡━━━╇━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0 │ loss                  │ ReverseDistillationLoss  │      0 │
│ 1 │ _transform            │ Compose                  │      0 │
│ 2 │ normalization_metrics │ MinMax                   │      0 │
│ 3 │ image_threshold       │ F1AdaptiveThreshold      │      0 │
│ 4 │ pixel_threshold       │ F1AdaptiveThreshold      │      0 │
│ 5 │ image_metrics         │ AnomalibMetricCollection │      0 │
│ 6 │ pixel_metrics         │ AnomalibMetricCollection │      0 │
│ 7 │ model                 │ ReverseDistillationModel │ 89.0 M │
└───┴───────────────────────┴──────────────────────────┴────────┘
Trainable params: 89.0 M
Non-trainable params: 0
Total params: 89.0 M
Total estimated model params size (MB): 356
/home/jinyao/anaconda3/envs/IADBE/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=63` in the `DataLoader` to improve performance.
/home/jinyao/anaconda3/envs/IADBE/lib/python3.10/site-packages/lightning/pytorch/core/module.py:494: You called `self.log('train_loss', ..., logger=True)` but have no logger configured. You can enable one by doing `Trainer(logger=ALogger(...))`
/home/jinyao/anaconda3/envs/IADBE/lib/python3.10/site-packages/lightning/pytorch/utilities/data.py:77: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 10. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.
/home/jinyao/anaconda3/envs/IADBE/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=63` in the `DataLoader` to improve performance.
Epoch 199/199 ━━━━━━━━━━━━━━━━ 10/10 0:00:08 •        1.11it/s train_loss_step:
                                     0:00:00                   0.084
                                                               train_loss_epoch:
                                                               0.085
Validation    ━━━━             1/4   0:00:00 •        0.00it/s
                                     -:--:--
Traceback (most recent call last):
  File "/home/jinyao/PycharmProjects/IADBE/train_test_mvtec3d_reverse_distillation.py", line 62, in <module>
    engine.fit(datamodule=datamodule, model=model)
  File "/home/jinyao/anaconda3/envs/IADBE/lib/python3.10/site-packages/anomalib/engine/engine.py", line 540, in fit
    self.trainer.fit(model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)
  File "/home/jinyao/anaconda3/envs/IADBE/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 544, in fit
    call._call_and_handle_interrupt(
  File "/home/jinyao/anaconda3/envs/IADBE/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py", line 44, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "/home/jinyao/anaconda3/envs/IADBE/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 580, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/home/jinyao/anaconda3/envs/IADBE/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 989, in _run
    results = self._run_stage()
  File "/home/jinyao/anaconda3/envs/IADBE/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 1035, in _run_stage
    self.fit_loop.run()
  File "/home/jinyao/anaconda3/envs/IADBE/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py", line 202, in run
    self.advance()
  File "/home/jinyao/anaconda3/envs/IADBE/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py", line 359, in advance
    self.epoch_loop.run(self._data_fetcher)
  File "/home/jinyao/anaconda3/envs/IADBE/lib/python3.10/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 137, in run
    self.on_advance_end(data_fetcher)
  File "/home/jinyao/anaconda3/envs/IADBE/lib/python3.10/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 285, in on_advance_end
    self.val_loop.run()
  File "/home/jinyao/anaconda3/envs/IADBE/lib/python3.10/site-packages/lightning/pytorch/loops/utilities.py", line 182, in _decorator
    return loop_run(self, *args, **kwargs)
  File "/home/jinyao/anaconda3/envs/IADBE/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py", line 134, in run
    self._evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)
  File "/home/jinyao/anaconda3/envs/IADBE/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py", line 405, in _evaluation_step
    call._call_callback_hooks(trainer, hook_name, output, *hook_kwargs.values())
  File "/home/jinyao/anaconda3/envs/IADBE/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py", line 208, in _call_callback_hooks
    fn(trainer, trainer.lightning_module, *args, **kwargs)
  File "/home/jinyao/anaconda3/envs/IADBE/lib/python3.10/site-packages/anomalib/callbacks/metrics.py", line 125, in on_validation_batch_end
    self._update_metrics(pl_module.image_metrics, pl_module.pixel_metrics, outputs)
  File "/home/jinyao/anaconda3/envs/IADBE/lib/python3.10/site-packages/anomalib/callbacks/metrics.py", line 185, in _update_metrics
    pixel_metric.update(torch.squeeze(output["anomaly_maps"]), torch.squeeze(output["mask"].int()))
  File "/home/jinyao/anaconda3/envs/IADBE/lib/python3.10/site-packages/anomalib/metrics/collection.py", line 26, in update
    super().update(*args, **kwargs)
  File "/home/jinyao/anaconda3/envs/IADBE/lib/python3.10/site-packages/torchmetrics/collections.py", line 220, in update
    m.update(*args, **m_kwargs)
  File "/home/jinyao/anaconda3/envs/IADBE/lib/python3.10/site-packages/torchmetrics/metric.py", line 492, in wrapped_func
    raise err
  File "/home/jinyao/anaconda3/envs/IADBE/lib/python3.10/site-packages/torchmetrics/metric.py", line 482, in wrapped_func
    update(*args, **kwargs)
  File "/home/jinyao/anaconda3/envs/IADBE/lib/python3.10/site-packages/anomalib/metrics/auroc.py", line 62, in update
    super().update(preds.flatten(), target.flatten())
  File "/home/jinyao/anaconda3/envs/IADBE/lib/python3.10/site-packages/torchmetrics/classification/precision_recall_curve.py", line 165, in update
    _binary_precision_recall_curve_tensor_validation(preds, target, self.ignore_index)
  File "/home/jinyao/anaconda3/envs/IADBE/lib/python3.10/site-packages/torchmetrics/functional/classification/precision_recall_curve.py", line 136, in _binary_precision_recall_curve_tensor_validation
    _check_same_shape(preds, target)
  File "/home/jinyao/anaconda3/envs/IADBE/lib/python3.10/site-packages/torchmetrics/utilities/checks.py", line 42, in _check_same_shape
    raise RuntimeError(
RuntimeError: Predictions and targets are expected to have the same shape, but got torch.Size([2097152]) and torch.Size([4718592]).

Process finished with exit code 1
